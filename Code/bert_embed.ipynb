{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r9GN4oM7OOFw",
        "outputId": "f6b03180-4216-4076-d82c-f6e6e3a1b9a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['arabic', 'azerbaijani', 'basque', 'bengali', 'catalan', 'chinese', 'danish', 'dutch', 'english', 'finnish', 'french', 'german', 'greek', 'hebrew', 'hinglish', 'hungarian', 'indonesian', 'italian', 'kazakh', 'nepali', 'norwegian', 'portuguese', 'romanian', 'russian', 'slovene', 'spanish', 'swedish', 'tajik', 'turkish']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\andre\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import jieba\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "print(stopwords.fileids())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8_ZaboUEOOFz"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('dataset/medical_cases.csv')\n",
        "for column in df:\n",
        "    # Replace missing values with a placeholder token\n",
        "    df[column].fillna('未知', inplace=True)\n",
        "# Tokenize multiple columns\n",
        "columns_to_tokenize =  ['睡眠', '大便','小便','胃口','渴', '手足',\t'頭身','汗','月經','其他','脈診', '望診','舌診','眼診',\t'特殊診斷', '耳診','診斷']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXbYsusROOGA"
      },
      "source": [
        "#### BERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZExmC2hqOOGB"
      },
      "outputs": [],
      "source": [
        "from transformers import BertModel, BertTokenizer\n",
        "import torch\n",
        "import string\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UuHAhc85OOGB",
        "outputId": "e53d581d-dbe1-45f1-8fa3-520874e27dcc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"\\n# Load tokenizer and model\\ntokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\\nmodel = BertModel.from_pretrained('bert-base-chinese')\\n\\n# Initialize a list to store embeddings\\nembeddings_dict = {col: [] for col in columns_to_tokenize}\\n\\nbatch_size = 100\\n\\nfor column in columns_to_tokenize:\\n    \\n    texts = df[column].tolist()\\n    embeddings_list = []\\n    \\n    for i in range(0, len(texts), batch_size):\\n        batch_texts = texts[i:i+batch_size]\\n        batch_no_punct = [text.translate(str.maketrans('', '', string.punctuation)) for text in batch_texts]\\n        stop_words = set(nltk.corpus.stopwords.words('chinese'))\\n        filtered_texts = [' '.join([word for word in nltk.word_tokenize(text) if word not in stop_words]) for text in batch_no_punct]\\n         # Tokenize the filtered text using BERT tokenizer\\n        encoded_input = tokenizer.batch_encode_plus(filtered_texts, max_length=512, padding='max_length', truncation=True, return_tensors='pt')\\n        # Obtain the embeddings\\n        with torch.no_grad():\\n            outputs = model(**encoded_input)\\n            \\n        # Get the embeddings of the [CLS] token (the first token)\\n        text_embedding = outputs.last_hidden_state[:, 0, :].numpy()\\n        # Append the embeddings to the list\\n        embeddings_list.append(text_embedding)\\n\\n    embeddings_dict[column] = np.concatenate(embeddings_list, axis=0)\\n\\n# Create df from embedding\\ndf_embeddings = pd.DataFrame.from_dict({key: np.array(value).tolist() for key, value in embeddings_dict.items()})\\n\""
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "# Load tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
        "model = BertModel.from_pretrained('bert-base-chinese')\n",
        "\n",
        "# Initialize a list to store embeddings\n",
        "embeddings_dict = {col: [] for col in columns_to_tokenize}\n",
        "\n",
        "batch_size = 100\n",
        "\n",
        "for column in columns_to_tokenize:\n",
        "\n",
        "    texts = df[column].tolist()\n",
        "    embeddings_list = []\n",
        "\n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch_texts = texts[i:i+batch_size]\n",
        "        batch_no_punct = [text.translate(str.maketrans('', '', string.punctuation)) for text in batch_texts]\n",
        "        stop_words = set(nltk.corpus.stopwords.words('chinese'))\n",
        "        filtered_texts = [' '.join([word for word in nltk.word_tokenize(text) if word not in stop_words]) for text in batch_no_punct]\n",
        "         # Tokenize the filtered text using BERT tokenizer\n",
        "        encoded_input = tokenizer.batch_encode_plus(filtered_texts, max_length=512, padding='max_length', truncation=True, return_tensors='pt')\n",
        "        # Obtain the embeddings\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**encoded_input)\n",
        "\n",
        "        # Get the embeddings of the [CLS] token (the first token)\n",
        "        text_embedding = outputs.last_hidden_state[:, 0, :].numpy()\n",
        "        # Append the embeddings to the list\n",
        "        embeddings_list.append(text_embedding)\n",
        "\n",
        "    embeddings_dict[column] = np.concatenate(embeddings_list, axis=0)\n",
        "\n",
        "# Create df from embedding\n",
        "df_embeddings = pd.DataFrame.from_dict({key: np.array(value).tolist() for key, value in embeddings_dict.items()})\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BQxVWGZOOOGD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UB7icPBsOOGD"
      },
      "outputs": [],
      "source": [
        "# Replace 'your_file_path' with the path to your pickle file\n",
        "df_pkl = pd.read_pickle('df_embed.pkl')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-HC6AWb7OOGE",
        "outputId": "832b9da7-cee3-40be-c03e-97ab8ba84136"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>睡眠</th>\n",
              "      <th>大便</th>\n",
              "      <th>小便</th>\n",
              "      <th>胃口</th>\n",
              "      <th>渴</th>\n",
              "      <th>手足</th>\n",
              "      <th>頭身</th>\n",
              "      <th>汗</th>\n",
              "      <th>月經</th>\n",
              "      <th>其他</th>\n",
              "      <th>脈診</th>\n",
              "      <th>望診</th>\n",
              "      <th>舌診</th>\n",
              "      <th>眼診</th>\n",
              "      <th>特殊診斷</th>\n",
              "      <th>耳診</th>\n",
              "      <th>診斷</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.005378</td>\n",
              "      <td>-0.005835</td>\n",
              "      <td>-0.005781</td>\n",
              "      <td>-0.004479</td>\n",
              "      <td>-0.005303</td>\n",
              "      <td>-0.005857</td>\n",
              "      <td>-0.006673</td>\n",
              "      <td>-0.006673</td>\n",
              "      <td>-0.006673</td>\n",
              "      <td>-0.004752</td>\n",
              "      <td>-0.005821</td>\n",
              "      <td>-0.006673</td>\n",
              "      <td>-0.006500</td>\n",
              "      <td>-0.005653</td>\n",
              "      <td>-0.006673</td>\n",
              "      <td>-0.004679</td>\n",
              "      <td>-0.005885</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.005988</td>\n",
              "      <td>-0.005155</td>\n",
              "      <td>-0.004955</td>\n",
              "      <td>-0.006673</td>\n",
              "      <td>-0.006673</td>\n",
              "      <td>-0.005620</td>\n",
              "      <td>-0.006673</td>\n",
              "      <td>-0.006673</td>\n",
              "      <td>-0.006673</td>\n",
              "      <td>-0.005058</td>\n",
              "      <td>-0.006673</td>\n",
              "      <td>-0.006673</td>\n",
              "      <td>-0.004583</td>\n",
              "      <td>-0.006749</td>\n",
              "      <td>-0.006673</td>\n",
              "      <td>-0.006673</td>\n",
              "      <td>-0.005185</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.007593</td>\n",
              "      <td>-0.005061</td>\n",
              "      <td>-0.005891</td>\n",
              "      <td>-0.005214</td>\n",
              "      <td>-0.004871</td>\n",
              "      <td>-0.006159</td>\n",
              "      <td>-0.006673</td>\n",
              "      <td>-0.005673</td>\n",
              "      <td>-0.006673</td>\n",
              "      <td>-0.005680</td>\n",
              "      <td>-0.003210</td>\n",
              "      <td>-0.006673</td>\n",
              "      <td>-0.004583</td>\n",
              "      <td>-0.006673</td>\n",
              "      <td>-0.006673</td>\n",
              "      <td>-0.006673</td>\n",
              "      <td>-0.004606</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.006411</td>\n",
              "      <td>-0.006021</td>\n",
              "      <td>-0.005456</td>\n",
              "      <td>-0.006673</td>\n",
              "      <td>-0.005069</td>\n",
              "      <td>-0.004503</td>\n",
              "      <td>-0.006673</td>\n",
              "      <td>-0.006673</td>\n",
              "      <td>-0.006673</td>\n",
              "      <td>-0.006537</td>\n",
              "      <td>-0.004993</td>\n",
              "      <td>-0.006673</td>\n",
              "      <td>-0.007379</td>\n",
              "      <td>-0.006673</td>\n",
              "      <td>-0.006673</td>\n",
              "      <td>-0.006673</td>\n",
              "      <td>-0.006350</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.005533</td>\n",
              "      <td>-0.005675</td>\n",
              "      <td>-0.004985</td>\n",
              "      <td>-0.004843</td>\n",
              "      <td>-0.004490</td>\n",
              "      <td>-0.006673</td>\n",
              "      <td>-0.005614</td>\n",
              "      <td>-0.004302</td>\n",
              "      <td>-0.006673</td>\n",
              "      <td>-0.004772</td>\n",
              "      <td>-0.004229</td>\n",
              "      <td>-0.006673</td>\n",
              "      <td>-0.006337</td>\n",
              "      <td>-0.005798</td>\n",
              "      <td>-0.006673</td>\n",
              "      <td>-0.006673</td>\n",
              "      <td>-0.006610</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         睡眠        大便        小便        胃口         渴        手足        頭身  \\\n",
              "0 -0.005378 -0.005835 -0.005781 -0.004479 -0.005303 -0.005857 -0.006673   \n",
              "1 -0.005988 -0.005155 -0.004955 -0.006673 -0.006673 -0.005620 -0.006673   \n",
              "2 -0.007593 -0.005061 -0.005891 -0.005214 -0.004871 -0.006159 -0.006673   \n",
              "3 -0.006411 -0.006021 -0.005456 -0.006673 -0.005069 -0.004503 -0.006673   \n",
              "4 -0.005533 -0.005675 -0.004985 -0.004843 -0.004490 -0.006673 -0.005614   \n",
              "\n",
              "          汗        月經        其他        脈診        望診        舌診        眼診  \\\n",
              "0 -0.006673 -0.006673 -0.004752 -0.005821 -0.006673 -0.006500 -0.005653   \n",
              "1 -0.006673 -0.006673 -0.005058 -0.006673 -0.006673 -0.004583 -0.006749   \n",
              "2 -0.005673 -0.006673 -0.005680 -0.003210 -0.006673 -0.004583 -0.006673   \n",
              "3 -0.006673 -0.006673 -0.006537 -0.004993 -0.006673 -0.007379 -0.006673   \n",
              "4 -0.004302 -0.006673 -0.004772 -0.004229 -0.006673 -0.006337 -0.005798   \n",
              "\n",
              "       特殊診斷        耳診        診斷  \n",
              "0 -0.006673 -0.004679 -0.005885  \n",
              "1 -0.006673 -0.006673 -0.005185  \n",
              "2 -0.006673 -0.006673 -0.004606  \n",
              "3 -0.006673 -0.006673 -0.006350  \n",
              "4 -0.006673 -0.006673 -0.006610  "
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Function to perform average pooling on a vector\n",
        "def average_pooling(vector):\n",
        "    return np.mean(vector, axis=0)\n",
        "\n",
        "for col in df_pkl.columns:\n",
        "    df_pkl[col] = df_pkl[col].apply(average_pooling)\n",
        "\n",
        "df_pkl.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mwXhf4DOOGE"
      },
      "source": [
        "minmax scale using sklearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fLn9pmuYOOGF",
        "outputId": "c8b0f688-6632-41d5-b06c-a5482efdecaa"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>睡眠</th>\n",
              "      <th>大便</th>\n",
              "      <th>小便</th>\n",
              "      <th>胃口</th>\n",
              "      <th>渴</th>\n",
              "      <th>手足</th>\n",
              "      <th>頭身</th>\n",
              "      <th>汗</th>\n",
              "      <th>月經</th>\n",
              "      <th>其他</th>\n",
              "      <th>脈診</th>\n",
              "      <th>望診</th>\n",
              "      <th>舌診</th>\n",
              "      <th>眼診</th>\n",
              "      <th>特殊診斷</th>\n",
              "      <th>耳診</th>\n",
              "      <th>診斷</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.516212</td>\n",
              "      <td>0.534519</td>\n",
              "      <td>0.336267</td>\n",
              "      <td>0.766479</td>\n",
              "      <td>0.516675</td>\n",
              "      <td>0.545134</td>\n",
              "      <td>0.273016</td>\n",
              "      <td>0.318367</td>\n",
              "      <td>0.18294</td>\n",
              "      <td>0.639240</td>\n",
              "      <td>0.351006</td>\n",
              "      <td>0.032707</td>\n",
              "      <td>0.347733</td>\n",
              "      <td>0.535022</td>\n",
              "      <td>0.141022</td>\n",
              "      <td>0.963181</td>\n",
              "      <td>0.408464</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.403403</td>\n",
              "      <td>0.674728</td>\n",
              "      <td>0.533405</td>\n",
              "      <td>0.124094</td>\n",
              "      <td>0.186676</td>\n",
              "      <td>0.590124</td>\n",
              "      <td>0.273016</td>\n",
              "      <td>0.318367</td>\n",
              "      <td>0.18294</td>\n",
              "      <td>0.588007</td>\n",
              "      <td>0.181971</td>\n",
              "      <td>0.032707</td>\n",
              "      <td>0.771303</td>\n",
              "      <td>0.317358</td>\n",
              "      <td>0.141022</td>\n",
              "      <td>0.308320</td>\n",
              "      <td>0.515476</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.106451</td>\n",
              "      <td>0.693951</td>\n",
              "      <td>0.309842</td>\n",
              "      <td>0.551085</td>\n",
              "      <td>0.620606</td>\n",
              "      <td>0.487817</td>\n",
              "      <td>0.273016</td>\n",
              "      <td>0.553165</td>\n",
              "      <td>0.18294</td>\n",
              "      <td>0.483565</td>\n",
              "      <td>0.869128</td>\n",
              "      <td>0.032707</td>\n",
              "      <td>0.771303</td>\n",
              "      <td>0.332448</td>\n",
              "      <td>0.141022</td>\n",
              "      <td>0.308320</td>\n",
              "      <td>0.603997</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.325122</td>\n",
              "      <td>0.496165</td>\n",
              "      <td>0.413697</td>\n",
              "      <td>0.124094</td>\n",
              "      <td>0.572864</td>\n",
              "      <td>0.801850</td>\n",
              "      <td>0.273016</td>\n",
              "      <td>0.318367</td>\n",
              "      <td>0.18294</td>\n",
              "      <td>0.339882</td>\n",
              "      <td>0.515369</td>\n",
              "      <td>0.032707</td>\n",
              "      <td>0.153719</td>\n",
              "      <td>0.332448</td>\n",
              "      <td>0.141022</td>\n",
              "      <td>0.308320</td>\n",
              "      <td>0.337405</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.487480</td>\n",
              "      <td>0.567533</td>\n",
              "      <td>0.526346</td>\n",
              "      <td>0.659810</td>\n",
              "      <td>0.712527</td>\n",
              "      <td>0.390382</td>\n",
              "      <td>0.483618</td>\n",
              "      <td>0.875251</td>\n",
              "      <td>0.18294</td>\n",
              "      <td>0.635907</td>\n",
              "      <td>0.666906</td>\n",
              "      <td>0.032707</td>\n",
              "      <td>0.383889</td>\n",
              "      <td>0.506147</td>\n",
              "      <td>0.141022</td>\n",
              "      <td>0.308320</td>\n",
              "      <td>0.297570</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         睡眠        大便        小便        胃口         渴        手足        頭身  \\\n",
              "0  0.516212  0.534519  0.336267  0.766479  0.516675  0.545134  0.273016   \n",
              "1  0.403403  0.674728  0.533405  0.124094  0.186676  0.590124  0.273016   \n",
              "2  0.106451  0.693951  0.309842  0.551085  0.620606  0.487817  0.273016   \n",
              "3  0.325122  0.496165  0.413697  0.124094  0.572864  0.801850  0.273016   \n",
              "4  0.487480  0.567533  0.526346  0.659810  0.712527  0.390382  0.483618   \n",
              "\n",
              "          汗       月經        其他        脈診        望診        舌診        眼診  \\\n",
              "0  0.318367  0.18294  0.639240  0.351006  0.032707  0.347733  0.535022   \n",
              "1  0.318367  0.18294  0.588007  0.181971  0.032707  0.771303  0.317358   \n",
              "2  0.553165  0.18294  0.483565  0.869128  0.032707  0.771303  0.332448   \n",
              "3  0.318367  0.18294  0.339882  0.515369  0.032707  0.153719  0.332448   \n",
              "4  0.875251  0.18294  0.635907  0.666906  0.032707  0.383889  0.506147   \n",
              "\n",
              "       特殊診斷        耳診        診斷  \n",
              "0  0.141022  0.963181  0.408464  \n",
              "1  0.141022  0.308320  0.515476  \n",
              "2  0.141022  0.308320  0.603997  \n",
              "3  0.141022  0.308320  0.337405  \n",
              "4  0.141022  0.308320  0.297570  "
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Create a StandardScaler object\n",
        "scaler = MinMaxScaler()\n",
        "# Apply Z-score normalization to each column containing vectors\n",
        "    # Fit and transform each column's data using StandardScaler\n",
        "scaled_data = scaler.fit_transform(df_pkl)\n",
        "\n",
        "# Create a new DataFrame with the scaled data\n",
        "df_scaled = pd.DataFrame(scaled_data, columns=df_pkl.columns)\n",
        "\n",
        "df_scaled.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPP-NhUEOOGF"
      },
      "source": [
        "z-score using scipy.stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2QEhU1J2OOGG"
      },
      "outputs": [],
      "source": [
        "# from scipy.stats import zscore\n",
        "\n",
        "# df_pkl_z = pd.DataFrame()\n",
        "# for col in df_pkl.columns:\n",
        "#     df_pkl_z[col] = zscore(df_pkl[col])\n",
        "\n",
        "# df_pkl_z.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AsNiE0TpOOGG"
      },
      "source": [
        "### Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "68fjfmxWOOGG",
        "outputId": "104dcb16-720c-4add-e1a8-b4a2ef85e4f8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nfrom sklearn.naive_bayes import GaussianNB\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\\ndf = pd.read_csv(\\'dataset/medical_cases.csv\\')\\ny_label = df[\\'白芍\\']\\n# Assuming y_label is your target labels for classification\\n# Split the data into training and testing sets\\nX_train, X_test, y_train, y_test = train_test_split(df_pkl_z, y_label, test_size=0.2, random_state=42)\\n\\n# Initialize and fit the Naive Bayes classifier\\nnb_classifier = GaussianNB()\\nnb_classifier.fit(X_train, y_train)\\n\\n\\n# Predict on the test data\\npredictions = nb_classifier.predict(X_test)\\n\\n# Evaluate the classifier\\naccuracy = accuracy_score(y_test, predictions)\\nprecision = precision_score(y_test, predictions, average=\\'weighted\\')\\nrecall = recall_score(y_test, predictions, average=\\'weighted\\')\\nf1 = f1_score(y_test, predictions, average=\\'weighted\\')\\nconf_matrix = confusion_matrix(y_test, predictions)\\n\\n# Print or use the evaluation metrics as needed\\nprint(f\"Accuracy: {accuracy}\")\\nprint(f\"Precision: {precision}\")\\nprint(f\"Recall: {recall}\")\\nprint(f\"F1 Score: {f1}\")\\nprint(f\"Confusion Matrix: {conf_matrix}\")\\n'"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "df = pd.read_csv('dataset/medical_cases.csv')\n",
        "y_label = df['白芍']\n",
        "# Assuming y_label is your target labels for classification\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(df_pkl_z, y_label, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and fit the Naive Bayes classifier\n",
        "nb_classifier = GaussianNB()\n",
        "nb_classifier.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "# Predict on the test data\n",
        "predictions = nb_classifier.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "precision = precision_score(y_test, predictions, average='weighted')\n",
        "recall = recall_score(y_test, predictions, average='weighted')\n",
        "f1 = f1_score(y_test, predictions, average='weighted')\n",
        "conf_matrix = confusion_matrix(y_test, predictions)\n",
        "\n",
        "# Print or use the evaluation metrics as needed\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1 Score: {f1}\")\n",
        "print(f\"Confusion Matrix: {conf_matrix}\")\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_UCOoaoCOOGH",
        "outputId": "2ae5f33f-14e2-4397-8350-76f1903720d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.6\n",
            "Precision: 0.5975609756097561\n",
            "Recall: 0.6125\n",
            "F1 Score: 0.6049382716049384\n",
            "Confusion Matrix: [[47 33]\n",
            " [31 49]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "df = pd.read_csv('dataset/medical_cases.csv')\n",
        "y_label = df['白芍']\n",
        "# Assuming y_label is your target labels for classification\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(df_scaled, y_label, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and fit the Naive Bayes classifier\n",
        "nb_classifier = GaussianNB()\n",
        "nb_classifier.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "# Predict on the test data\n",
        "predictions = nb_classifier.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "precision = precision_score(y_test, predictions)\n",
        "recall = recall_score(y_test, predictions)\n",
        "f1 = f1_score(y_test, predictions)\n",
        "conf_matrix = confusion_matrix(y_test, predictions)\n",
        "\n",
        "# Print or use the evaluation metrics as needed\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1 Score: {f1}\")\n",
        "print(f\"Confusion Matrix: {conf_matrix}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ef5IOChiOOGH"
      },
      "source": [
        "### Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BPmksJPsOOGI",
        "outputId": "e207388b-c08a-4a47-bdb2-b9833b785f40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\andre\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
            "\n",
            "WARNING:tensorflow:From c:\\Users\\andre\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From c:\\Users\\andre\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\andre\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\engine\\data_adapter.py:1808: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n",
            "  return t[start:end]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\andre\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
            "\n",
            "WARNING:tensorflow:From c:\\Users\\andre\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
            "\n",
            "18/18 [==============================] - 2s 17ms/step - loss: 0.6862 - accuracy: 0.5305 - val_loss: 0.6538 - val_accuracy: 0.6875\n",
            "Epoch 2/20\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6820 - accuracy: 0.5602 - val_loss: 0.6538 - val_accuracy: 0.6719\n",
            "Epoch 3/20\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6788 - accuracy: 0.5812 - val_loss: 0.6474 - val_accuracy: 0.7031\n",
            "Epoch 4/20\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6778 - accuracy: 0.5410 - val_loss: 0.6688 - val_accuracy: 0.5781\n",
            "Epoch 5/20\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.6782 - accuracy: 0.5759 - val_loss: 0.6417 - val_accuracy: 0.6562\n",
            "Epoch 6/20\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.6828 - accuracy: 0.5515 - val_loss: 0.6591 - val_accuracy: 0.6250\n",
            "Epoch 7/20\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.6748 - accuracy: 0.5620 - val_loss: 0.6506 - val_accuracy: 0.6719\n",
            "Epoch 8/20\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.6706 - accuracy: 0.5916 - val_loss: 0.6518 - val_accuracy: 0.6250\n",
            "Epoch 9/20\n",
            "18/18 [==============================] - 0s 7ms/step - loss: 0.6703 - accuracy: 0.5742 - val_loss: 0.6509 - val_accuracy: 0.6094\n",
            "Epoch 10/20\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.6744 - accuracy: 0.5899 - val_loss: 0.6760 - val_accuracy: 0.5312\n",
            "Epoch 11/20\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.6716 - accuracy: 0.5812 - val_loss: 0.6439 - val_accuracy: 0.6250\n",
            "Epoch 12/20\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.6653 - accuracy: 0.5724 - val_loss: 0.6597 - val_accuracy: 0.5469\n",
            "Epoch 13/20\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.6708 - accuracy: 0.5829 - val_loss: 0.6629 - val_accuracy: 0.5312\n",
            "Epoch 14/20\n",
            "18/18 [==============================] - 0s 5ms/step - loss: 0.6525 - accuracy: 0.6038 - val_loss: 0.6511 - val_accuracy: 0.5781\n",
            "Epoch 15/20\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6575 - accuracy: 0.5881 - val_loss: 0.6477 - val_accuracy: 0.5938\n",
            "Epoch 16/20\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6742 - accuracy: 0.5567 - val_loss: 0.6327 - val_accuracy: 0.6562\n",
            "Epoch 17/20\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6407 - accuracy: 0.6265 - val_loss: 0.6746 - val_accuracy: 0.5312\n",
            "Epoch 18/20\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6519 - accuracy: 0.5986 - val_loss: 0.6598 - val_accuracy: 0.5000\n",
            "Epoch 19/20\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6478 - accuracy: 0.6283 - val_loss: 0.7133 - val_accuracy: 0.5000\n",
            "Epoch 20/20\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.6612 - accuracy: 0.5969 - val_loss: 0.6334 - val_accuracy: 0.6719\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 0.7246 - accuracy: 0.5125\n",
            "Test Accuracy: 51.25%\n",
            "5/5 [==============================] - 0s 1ms/step\n",
            "Test Accuracy: 0.5125\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
        "\n",
        "# Create a Sequential model\n",
        "model = Sequential()\n",
        "\n",
        "# Add layers to the model\n",
        "model.add(Flatten(input_shape=(X_train.shape[1],)))  # Input layer\n",
        "model.add(Dense(128, activation='relu'))  # Hidden layer with 128 neurons and ReLU activation\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(64, activation='relu'))  # Hidden layer with 64 neurons and ReLU activation\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))  # Output layer with 1 neuron for binary classification\n",
        "\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.1)\n",
        "# Evaluate the model on test data\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# Evaluate the model on test data\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_classes = (y_pred > 0.5).astype(\"int32\")  # Convert probabilities to classes\n",
        "accuracy = accuracy_score(y_test, y_pred_classes)\n",
        "\n",
        "print(f\"Test Accuracy: {accuracy}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yDK3kgoOOGI"
      },
      "source": [
        "### Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aJWijK6-OOGI",
        "outputId": "ca00a293-5e0a-44c3-f4d3-4f5581639468"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.58125\n",
            "Precision: 0.591694586492682\n",
            "Recall: 0.58125\n",
            "F1 Score: 0.5689759157251416\n",
            "Confusion Matrix: [[33 47]\n",
            " [20 60]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "\n",
        "# Assuming df_pkl contains your feature vectors and y_label is the target labels\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(df_scaled, y_label, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Random Forest classifier\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)  # You can adjust n_estimators and other hyperparameters\n",
        "\n",
        "# Train the classifier\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test data\n",
        "predictions = rf_classifier.predict(X_test)\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "precision = precision_score(y_test, predictions, average='weighted')\n",
        "recall = recall_score(y_test, predictions, average='weighted')\n",
        "f1 = f1_score(y_test, predictions, average='weighted')\n",
        "conf_matrix = confusion_matrix(y_test, predictions)\n",
        "\n",
        "# Print or use the evaluation metrics as needed\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1 Score: {f1}\")\n",
        "print(f\"Confusion Matrix: {conf_matrix}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}